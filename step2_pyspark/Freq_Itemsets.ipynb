{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Freq_Itemsets.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPLi3KQcKr4hKb//IwnZcO9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mann-brinson/valkyrie_interview/blob/main/step2_pyspark/Freq_Itemsets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvSMncECdf2c"
      },
      "source": [
        "#Mount Google Drive to access data files\n",
        "from google.colab import drive, files\n",
        "\n",
        "#Load the order_products_train_many.json input file\n",
        "drive.mount('/content/gdrive')\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wzFzSbjfb18"
      },
      "source": [
        "#Paramters\n",
        "input_file = \"order_products_train_many.json\"\n",
        "output_freq_itemsets = \"freq_itemsets.json\"\n",
        "output_item_baskets = \"item_baskets.json\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "431AyflHhfL2"
      },
      "source": [
        "#Install pyspark dependencies on Google Colab\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Install dependencies: jdk-8, hadoop-2.7, spark-3.1.1\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop2.7\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaQYQs2kiwgn"
      },
      "source": [
        "#Run a local spark session\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "#Apriori algorithm library\n",
        "from pyspark.ml.fpm import FPGrowth\n",
        "\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"Valkyrie\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfzZ9kXvBJfn"
      },
      "source": [
        "**Compute frequent itemsets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJdqYy2Fi3SI"
      },
      "source": [
        "#Load the json file of orders containing order_products purchased by users who made at least 20 orders\n",
        "jsonDF = spark.read.json(input_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1HYT96RCj2Q",
        "outputId": "3b1f8dfe-285e-443f-8b6b-8376b0818e9a"
      },
      "source": [
        "#Call the FPGrowth library\n",
        "\n",
        "#The minimum support for an itemset to be identified as frequent. \n",
        "#For example, if an item appears 3 out of 5 transactions, it has a support of 3/5=0.6.\n",
        "minSupport = 0.001\n",
        "\n",
        "#minimum confidence for generating Association Rule. Confidence is an indication of how often an association rule \n",
        "# has been found to be true. For example, if in the transactions itemset X appears 4 times, X and Y co-occur only 2 times, \n",
        "# the confidence for the rule X => Y is then 2/4 = 0.5. The parameter will not affect the mining for frequent itemsets, \n",
        "# but specify the minimum confidence for generating association rules from frequent itemsets.\n",
        "minConfidence = 0\n",
        "\n",
        "fpGrowth = FPGrowth(itemsCol=\"basket_content\", minSupport=minSupport, minConfidence=minConfidence)\n",
        "model = fpGrowth.fit(jsonDF)\n",
        "\n",
        "# Display frequent itemsets.\n",
        "model.freqItemsets.show(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+----+\n",
            "|               items|freq|\n",
            "+--------------------+----+\n",
            "|             [36695]| 295|\n",
            "|      [36695, 40706]|  41|\n",
            "|       [36695, 5876]|  34|\n",
            "|      [36695, 21137]|  77|\n",
            "|[36695, 21137, 13...|  33|\n",
            "|      [36695, 47626]|  45|\n",
            "|      [36695, 47209]|  82|\n",
            "|[36695, 47209, 13...|  46|\n",
            "|      [36695, 27966]|  60|\n",
            "|      [36695, 13176]|  97|\n",
            "+--------------------+----+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7RpN9nYIwQI"
      },
      "source": [
        "#Convert pyspark.sql.dataframe to pandas.dataframe\n",
        "pandasDF = model.freqItemsets.toPandas()\n",
        "\n",
        "#Convert from dataframe to dict\n",
        "freq_items_dict = pandasDF.to_dict('records')\n",
        "print(freq_items_dict)\n",
        "\n",
        "#Add the length attribute and then serialize as json \n",
        "freq_items_dict2 = dict()\n",
        "f_itemset_id = 1\n",
        "for freq_itemset in freq_items_dict:\n",
        "  len_itemset = len(freq_itemset['items'])\n",
        "  freq_items_dict2[f_itemset_id] = {'length': len_itemset, 'freq': freq_itemset['freq'], 'items': freq_itemset['items']}\n",
        "  f_itemset_id += 1\n",
        "\n",
        "#Save the result to a json file\n",
        "with open(output_freq_itemsets, 'w') as outfile:\n",
        "    json.dump(freq_items_dict2, outfile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCiQaHUYM-wK"
      },
      "source": [
        "#Download the resulting json file, then upload to DynamoDB\n",
        "#TODO: Configure DynamoDB Access Key in Google Colab, to put the JSON file in DynamoDB"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EomHuOLWSTev"
      },
      "source": [
        "#Create an item_baskets dict\n",
        "#For each item, in each itemset, generate a value for that item-key\n",
        "item_baskets = dict()\n",
        "for f_itemset_id in freq_items_dict2.keys():\n",
        "  for item in freq_items_dict2[f_itemset_id][\"items\"]:\n",
        "    if item not in item_baskets:\n",
        "      item_baskets[item] = {'itemset_ids': [f_itemset_id]}\n",
        "    else:\n",
        "      item_baskets[item]['itemset_ids'].append(f_itemset_id)\n",
        "  \n",
        "#Save the result to json file\n",
        "with open(output_item_baskets, 'w') as outfile:\n",
        "    json.dump(item_baskets, outfile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caW1kLnpTvEu"
      },
      "source": [
        "#Download the resulting json file, then upload to DynamoDB\n",
        "#TODO: Configure DynamoDB Access Key in Google Colab, to put the JSON file in DynamoDB"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YkjgM45UMGl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}